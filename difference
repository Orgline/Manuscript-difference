\documentclass[sigconf]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath}
%\usepackage{cite}
%\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{subcaption}
%\VerbatimFootnotes
\usepackage{layouts}
\usepackage{booktabs}
\usepackage{url}

\title{Recursion Brings Speedup to Out-of-Core TensorCore-based Linear Algebra Algorithms: A Case Study of Classic Gram-Schmidt QR factorization}
% \author{\IEEEauthorblockN{Shaoshuai Zhang, Panruo Wu}
% \IEEEauthorblockA{\textit{Department of Computer Science} \\
% \textit{University of Houston, Texas, USA}\\
% \{szhang36,pwu7\}@uh.edu}
% }
% \date{November 2020}

\author{Blind Review}
% \email{szhang36@uh.edu}
% \affiliation{%
%   \institution{University of Houston\\Department of Computer Science}
%   \city{Houston}
%   \state{TX}
%   \country{USA}
% }


\begin{document}




\newcommand{\flops}{\mbox{\#flops}}
\newcommand{\fromto}{$\leftrightarrow$}

% \section{notes: why recursive algorithm is better than non-recursive algorithm}
% \subsection{The total amount of data movement}
% Theoretical: the rough amount of data movement can be computed in section~\ref{datamovement}.
% \\
% Experimental: can be proved by \verb|nvprof|.

% \subsection{Inner product GEMM}
% $R_{12}=Q^TA_2$.
% \\
% In the recursive algorithm, the size will be $A(m,k)$ and $B(k,n)$, where $m=n$. To overlap data movement, we need the time cost of data movement $\frac{4mk+4nk}{12GB/s}$ to be less than the time cost of GEMM $\frac{2mnk}{90TFLOPs}$, then we have $\frac{1}{m}+\frac{1}{n}<\frac{1}{15000}$, the maximum $m$ in recursive algorithm is 65536 and it's possible to overlap the data movement.
% \\
% In the non-recursive algorithm, the data movement cannot be overlapped definitely, because the $m$ is very small. The strategy is to let the $A$ stay in memory, and use streams to process $B$. The the time cost of data movement is $\frac{4kb}{12GB/s}$ and the time cost of GEMM is $\frac{4mkb}{90TFLOPs}$, so the $m$ should be larger than 15000. On some GPUs, the $m$ cannot be so large to avoid running out of memory. But in recursive algorithm, we can control the blocksize to be small and this won't change the overlap rate. Besides, let the $A$ stay in memory still costs a lot, and this part cannot be overlapped at all.

% \subsection{Outer product GEMM}
% $A_2=A_2-Q_1R_{12}$
% \\
% The strategy will be different, as the size and shape is different. Now the size becomes $A(m,k)$ and $B(k,n)$. In recursive algorithm, the $m=2k=2n$, while in non-recursive algorithm, the $m$ is much larger than $k$. We will also need $C$ to be moved between host and device. The strategy for the recursive algorithm is like this: let $B$ stored in memory, then we scan $A$ from top to bottom with block size $b$, then the data movement cost is $\frac{4bk+4bn}{12GB/s}$ (host to device), and the GEMM cost is $\frac{2bkn}{90TFLOPs}$, so that $n$ should larger than 30000 to overlap data movement. It's very possible.
% \\
% For non-recursive algorithm, let $A$ and $B$ stay in memory, then we move $C$ in and out. Let the block size to be $b_a, b_b$, then the data movement cost is $\frac{4b_ab_b}{12GB/s}$ and the GEMM costs $\frac{2b_ab_bk}{90TFLOPs}$ so that $k$ should larger than $15000$. It has the some problem as the problem in inner product GEMM.

% \subsection{Conclusion}
% From the analysis above, the parameters of non-recursive GEMM is very critical and cannot be changed casually, actually, it's the blocksize of the entire QR factorization. We expect the blocksize to be larger to overlap the data movements in GEMMs, but we sometimes we cannot do that because the limited GPU memory. But for the recursive algorithm, the parameters is no that critical. For instance, in inner product GEMM, the blocksize can be adjusted casually due to the GPU memory, so that the data movement can still be overlapped. In outer product GEMM, the $n$ cannot be very small, but we can let a part of $B$ stay in memory without invading the overlap rate.

% \subsection{GEMM comparison methods}
% Show the performance is optimal theoretically.
% \\
% With BLASX.
% \\
% Timeline of recursive and non-recursive GEMM.

\begin{abstract}
   Out-of-core processing aims to handle large amount of data when the memory is limited. There exists several out-of-core applications including disk-memory and CPU-GPU processing. Ideally, these out-of-core applications can be expected to be close to the peak performance of the in-core computations, if the data movement between different memory hierarchies can be overlapped by the in-core computations effectively. However, with the emergence of matrix accelerators such as TensorCore GPU, the imbalance between the speed of computations and data movement is further exacerbated, such that even high computation intensity kernels can be dominated by data movement cost. In such cases, the algorithms need to be redesigned to reduce communication volume and overlap the data movement by pipelines. In this paper, we select classic Gram-Schmidt QR factorization as an example to illustrate our recursive strategy, which shows smaller amount of data movement and higher overlapping ratio than the conventional blocking QR factorization algorithm. The results suggest this technique can potentially be applied to broader matrix computations kernels. 
\end{abstract}
\maketitle

\section{Introduction}
Out-of-core (OOC) means out of memory (when main memory was called
core memory a long time ago). However,
OOC has received less attention than distributed parallel computing but we think OOC has its use cases in the era of data-centric computing.
Both can enable solving bigger problems that cannot fit in the memory
of a single node, but the latter also entails using multiple processing
units. In the era of vector and matrix accelerators, we think OOC is becoming more
important as it enables an increasingly common use case because of
the convenience of using a single node and the ubiquitous accessibility of such
platform. Furthermore, for
some problems, a single accelerator can rival the speed of a
medium sized cluster of CPUs. For example, a single A100 GPU with TensorCore
has a peak performance at around 300~TFLOPS, which is equal to about thousands of modern
high-end CPU cores with AVX512 vector units. Yet, the memory capacity of
the accelerator is often very limited. 
The limited problem size due to insufficient memory capacity greatly
limits the potential of the extremely fast accelerator. 

A particular class of problems that potentially benefits from
OOC with accelerators is dense linear algebra.  In this paper, we explore
techniques to enable efficient OOC computing on extremely fast accelerators such that the computing is no longer limited by scarce GPU memory. The key challenge
is the management of data movement.  It has been shown that the
optimal data movement volume is asymptotically $O(\frac{\flops}{\sqrt{M}})$ \cite{ballard2011minimizing}
where $\flops{}$ is
the amount of floating-point operations needed
and $M$ is the number of words in fast memory. 
Assuming the computation speed is $R_2$ (\#flops/s)
and communication speed is $R_1$ (\#words/s), 
the time spent moving data is
$$ O(\frac{\flops}{\sqrt{M} R_1}) $$
while the time spent doing computation is
$$ O(\frac{\flops}{R_2})$$

Now with the TensorCore unit (a.k.a. neural engines,
neural processors, or matrix accelerators)
the ratio of matrix computation speed and data
movement speed is getting extremely high. 
On a
modern Volta or Ampere NVIDIA GPU cards with TensorCore
as matrix computing units, 
the $R_2$ can be $10^{14}$, while 
$\sqrt{M}R_1$ is $10^{5} \times 10^{9.5}=10^{14.5}$. 
Assuming an algorithm
achieves optimal data movement, the cost of data movement is
comparable to computation. Such trend is likely going to continue. 
With a suboptimal data movement algorithm the communication
may dominate computation. In this situation, it's critically important
to minimize data movement and increase overlapping between
computation and communication, while at the same time without increasing
\flops{} or decreasing the execution rate $R_2$ significantly. 

There has been a long history and interest in OOC computing for linear algebra algorithms. And several similar OOC applications exist that work for CPU-GPU hierarchy. For instance, BLASX~\cite{wang2016blasx} and cuBLASXt provides OOC high-performance BLAS3 operations and some other specific applications such as OOC SVD~\cite{lu2020reducing} and~\cite{kabir2017framework}. However, over a long
period of accumulating faster computation speed growth over data movement speed
and with the advent of specialized matrix accelerator in addition to vector
accelerator, 
the balance between computing and communication is further skewed. As a result, the conventional design pattern of OOC algorithms becomes inadequate to hide
the cost of data movement, even with the abundance of computation intensity
present in dense linear algebra operations. 

To make the discussion concrete, we use NVIDIA TensorCore as an example
of a matrix accelerator. On NVIDIA Volta and Ampere architectures,
matrix computations can not only be carried out by CUDA cores (SGEMM), but also by TensorCore units (TC-GEMM). 
On Nvidia V100 GPU, CUDA core SGEMM usually performs at 14 TFLOPS, whereas TensorCore TC-GEMM can reach 112 TFLOPS, representing an 8x speedup by using the matrix accelerator. 
On the newer A100 GPU, the ratio is even higher at 16x. 
But the time cost of data movement between host and device and memory capacity do not
improve commensurately. 
Consequently, the algorithms used by state-of-the-art works such as BLASX~\cite{wang2016blasx}
is no longer adequate to hide all the communication costs. 
For instance, in terms of the V100 PCIe graphic card, the peak rate of data movement is around 13GB/s, which means moving two matrices with a size $16384*16384$ from host to device will take around 150ms. To put it into context, the computation using SGEMM will take 630ms while using TC-GEMM will take 97ms. It is evident that data movement
will very likely become the bottleneck. 


In view of the new situation, the TensorCore-based OOC application optimization needs
to be data movement centric. Without careful arrangement of data access patterns, the overall performance will be dominated by the data movement rather than the computations. So, there are two main challenges to be tackled to design high-performance OOC linear algebra algorithms. 1)How to reduce the data movement between different memory levels; 2)How to overlap the data movement with computations.

To illustrate how to address these challenges, we select QR factorization as an example, as it has a well-implemented in-core TensorCore-based algorithm~\cite{QRTensorCore}. And in the rest of this paper, we will try to explain why the conventional blocking algorithms cannot work efficiently and why the recursive algorithms have better performance. Besides, we will also discuss the details of how to implement the OOC QR factorization with data movement perfectly overlapped. We consider our contributions to be:
% Assuming a 8GB accelerator memory capacity, the largest problem size that
% can be handled is only 40K. Assuming 64GB CPU memory, the largest problem size
% that can be handled is about 100K. Assuming 1TB SSD disk, the largest problem
% size is 400K. Our goal is to handle the size up to that of the disk, with
% the speed of the TensorCore GPU. 

% Central challenges: 
% \begin{enumerate}
%     \item How to manage the multi-level data movement: disk \fromto{} CPU memory \fromto{} GPU memory
%     \item Pipeline to overlap communication and computation
%     \item Code reuse; algorithm adaption; 
% \end{enumerate}
\begin{itemize}
    \item Our algorithmic analysis, performance modeling, and empirical demonstration
    suggest that conventional blocking linear algebra algorithm
    is bottlenecked by either the data movement for fundamental matrix computation such as matrix-multiplication and QR factorization on modern
    matrix accelerator architectures, or the inefficient GEMMs due to the special shapes restricted by the fixed blocksize.
    
    \item We design a novel CPU-GPU hybrid computation module, data movement management, and pipeline based on recursive formulation
    of matrix-multiplication and QR factorization, which nearly perfectly overlap bidirectional data movement and highly efficient in-core computation, thereby the inside OOC TC-GEMMs can be said to be optimal.
    
    \item We demonstrate an end-to-end out-of-core QR factorization algorithm whose performance is up to 2x faster than conventional blocking QR factorization algorithm by
    comparison, and achieve around 45\% of TensorCore peak performance. 
\end{itemize}

The rest of the paper is organized as follows: Section II talks about the related work; section III analyzes the performance behaviors of the blocking and recursive algorithms; section IV explains the implementation and optimization details and section V provides experimental evaluations. And section VI draws a conclusion and depicts the future work.

\section{Related Work}
The out-of-core processing has a long history, but there's an approximate chronological boundary that before the year 2000, the out-of-core processing is usually deployed on a single disk-CPU hybrid system, while after the year 2000, it's often used on CPU-GPU hybrid system.  In the late 2000s, the development of GPGPU and distributed systems attracts many researchers to explore how to use GPU to deal with a massive scale of data and thereby leads to the usage of out-of-core processing on CPU-GPU hybrid system and distributed systems.  Since 2017 various devices such as
NVIDIA TensorCore, Google Tensor Processing Unit (TPU), and so
on are quickly emerging in response to the high demand for neural
network training and inference tasks. The speed of the matrix
multiplication becomes so fast, that the biggest problem fit
into the device memory would only take a second
to finish. However without out-of-core capability, such
impressive speed cannot be used to solve a larger problem that
does not fit into the very limited device memory space. 
Highly efficient out-of-core matrix computations are highly
desirable due to the accessibility and simplicity of a single
machine. 

\subsection{Disk-CPU Out-of-Core}
In 1996, the SOLAR~\cite{toledo1996design} was released and it became the first well-implemented out-of-core package. It combines LAPACK~\cite{anderson1999lapack} and ScaLAPACK~\cite{blackford1997scalapack} based in-core subroutines, out-of-core subroutines and input-output subroutines. Later in 2002, Toledo and Rabani~\cite{toledo2002very} proposed an out-of-core filter-diagonalization method to solve the very large electronic structure calculations. Note that they also proposed an out-of-core recursive QR factorization, which is not included in the SOLAR package, in this work.

With the development of distributed systems, researchers are more interested in solving large problems on distributed machines or multiple threads architecture rather than on a single machine. As a result, out-of-core processing is investigated on distributed systems. D'Azvedo and Dongarra~\cite{d2000design} migrated LU, QR, and Cholesky subroutines in ScaLAPACK to out-of-core relative subroutines. And in 2012, Gregorio et al.~\cite{quintana2012runtime} designed some out-of-core subroutines that don't lose any performance on multithreaded architectures.

\subsection{CPU-GPU Out-of-Core}
cuBLASXt~\footnote{\url{https://docs.nvidia.com/cuda/cublas/index.html\#using-the-cublasXt-api}} library offered by Nvidia provides hybrid CPU-GPU implementations of BLAS3 routines. In 2016, BLASX~\cite{wang2016blasx} used different out-of-core strategy and the performance is slightly better than cuBLASXt. In addition, both cuBLASXt and BLASX aims at the multiple-GPUs platform.

There're also some other CPU-GPU OOC applications that solve specific problems. For example, both\cite{kabir2017framework} and\cite{lu2020reducing} discusses the OOC SVD implementation on CPU-GPU hybrid system.

\subsection{TensorCore Technology}
In 2017, TensorCore technology was introduced by Nvidia on its Tesla architecture\cite{nvidia2017Tesla}. There're some investigations~\cite{markidis2018TensorCorePerformance}, micro-architecture analysis and benchmarking~\cite{jia2018dissectingTensorCore}. Except half precision GEMMs, researchers also indicate that TensorCore can also be used for reduction and scan~\cite{dakkak2019TensorCoreScan}. In \cite{TensorCoreLinearAlgebra1,TensorCoreLinearAlgebra2,LUTensorCore}, TensorCore was used for accelerating linear system solvers in the framework of hybrid CPU/GPU linear algebra package MAGMA\cite{dongarra2014MAGMA}; and TensorCore based QR factorization~\cite{QRTensorCore} mentioned before.

\subsection{Recursive Linear Algebra Algorithms}
The recursive linear algebra algorithms have long been
known to the numerical algorithm communities and have been
shown to be advantageous over blocking algorithms, 
however such performance gains are typically rather small, as blocking alone was
able to achieve near peak performance on previous architectures. 
Examples include recursive Cholesky and LU proposed in Lawra~\cite{lawra} and ReLAPACK~\cite{ReLAPACK},
recursive partial pivoted LU by Toledo~\cite{toledo1997locality}.
Recursive QR factorization has been studied by \cite{RecursiveQR}
as panel factorization solver 
and extended to full matrix factorization to TensorCore~\cite{QRTensorCore}. Recently, Zhang has proposed some TensorCore recursive BLAS3 algorithms~\cite{zhang2020basic} that can be used in matrix factorization. A recursive version 
of the Linpack benchmark (LU factorization) is studied on
iPad 2 in~\cite{dongarra2012anatomy}. 

% \subsection{Recursive }

% \subsection{Important references}
% \paragraph{A Runtime System for Programming Out-of-Core Matrix Algorithms-by-Tiles on Multithreaded Architectures~\cite{quintana2012runtime}}
% Summarize here. 

% \paragraph{Reducing the amount of out-of-core data access for GPU-accelerated randomized SVD~\cite{lu2020reducing}}

% summarize

% \paragraph{A framework for out of memory svd algorithms \cite{kabir2017framework}}

% summarize

% \paragraph{BLASX: A High Performance Level-3 BLASLibrary for Heterogeneous Multi-GPU Computing~\cite{wang2016blasx}}:

% summarize

% \paragraph{(SOLAR?) Very Large Electronic StructureCalculations Using an Out-of-CoreFilter-Diagonalization Method~\cite{toledo2002very}}

% summarize

% \paragraph{The design and implementation of the parallel out-of-core ScaLAPACK LU, QR, and Cholesky factorization routines\cite{d2000design}}


\section{Performance analysis}
In this section, we'll analyze the performance of the recursive algorithm and compare it with the blocking algorithm. We'll discuss the amount of data movement and the overlapping behaviors of OOC TCGEMMs. We'll also give a brief introduction to QR factorization to make the analysis more understandable.

\subsection{QR factorization}
The QR factorization, also known as QR decomposition, aims to factorize a matrix $A$ into a product of an orthogonal matrix $Q$ and an upper triangular matrix $R$. It's widely used in the scientific and engineering area to solve orthogonalization, linear least square problems, eigenvalue decomposition, and singular value decomposition problems. In general, there are three common-used algorithms, including Gram-Schmidt, Householder, and Givens rotation. In this paper, we use the classic Gram-Schmidt to illustrate our methodology, thus we'll only give the background knowledge of the classic Gram-Schmidt QR factorization algorithm.

\subsubsection{Gram-Schmidt process}
The Gram-Schmidt process aims to find a set of orthonormal vectors in an inner product space. Given an array of linearly independent vectors $[a_1|a_2|a_3|...|a_n]$, the Gram-Schmidt will find the orthonormal basis one by one. Then the set of orthonormal vectors $[q_1|q_2|q_3|...|q_n]$ is given by Equation~(\ref{eq:gramSchmidt}).
\small
\begin{equation}
    \begin{aligned}
    u_1&=a_1,&q_1={u_1}/{||u_1||}
    \\
    u_2&=a_2-\text{Proj}_{u_1}(a_2), &q_2={u_2}/{||u_2||}
    \\
    u_3&=a_3-\text{Proj}_{u_1}(a_3)-\text{Proj}_{u_2}(a_3),&q_3={u_3}/{||u_3||}
    \\
    &\vdots &\vdots\ \ \ \ 
    \\
   u_n&=a_n-\sum\limits_{j=1}^{n-1}\text{Proj}_{u_j}(a_n),&q_n={u_n}/{||u_n||}
    \end{aligned}
    \label{eq:gramSchmidt}
\end{equation}\normalsize
in which $\text{Proj}_{u}(a) = uu^Ta=u(u^Ta)$ is the orthogonal projection
of the vector $a$ onto an unit vector $u$. 
We also obtain $R$ during the Gram-Schmidt process: $r_{ij} = q_i^Ta_i$. 
% \small\begin{equation}
%     \begin{aligned}
%         \overbrace{
%         \underbrace{
%         \left[
%         \begin{matrix}
%             q_1 & q_2 & \cdots & q_n
%         \end{matrix}
%         \right]}_{Q}
%         \underbrace{
%         \left[
%         \begin{matrix}
%             q_1^T a_1 & e_1\cdot v_2 & \cdots & e_1\cdot v_n \\
%             0 & e_2\cdot v_2 & \cdots & e_2\cdot v_n\\
%             \vdots & \vdots & \ddots & \vdots \\
%             0 & 0 & \cdots & e_n\cdot v_n
%         \end{matrix}
%         \right]}_{R}
%         }^{A=\left[
%         \begin{matrix}
%             a_1 & a_2 & \cdots & a_n
%         \end{matrix}
%         \right]}
%     \end{aligned}
%     \label{eq:gsqr}
% \end{equation}\normalsize
Depending on the evaluation order of the procedure described in
\eqref{eq:gramSchmidt}, there are two mathematically equivalent but
numerically different variants called classic Gram-Schmidt (CGS)
and modified Gram-Schmidt (MGS). CGS executes row by row in \eqref{eq:gramSchmidt}, whereas MGS subtracts $\text{Proj}_{u_i}(a_j)$
from $a_j$ for all $j>i$ as soon as $u_i$ is computed. MGS can
be visualized as evaluating \eqref{eq:gramSchmidt} with row and column
interleaved. This subtle difference has important implications in numerical
stability and parallelism exposed. MGS is more stable but less parallel. 

As described above, the Gram-Schmidt process is inefficient on hierarchical memory
systems due to the low data locality of the vector-matrix operations. To improve
locality, blocking must be applied such that the orthogonal projection of multiple vectors onto multiple orthonormal vectors is performed in one shot. CGS can be trivially
blocked since it can be directly ``upgraded'' into blocking algorithm by
considering the vectors $a_i, u_i$ not as vectors but as a block matrix (a group
of column vectors). MGS, on the other hand, is not obvious to block. 

\subsubsection{Blocking Strategy}

The blocking QR factorization's workflow is shown in Fig~\ref{fig:bl_qr}. At first, we select a blocksize $b$ and factorize the first $b$ columns (termed as panel). Then we have the factorized $Q_1$ and let it time the rest of the matrix $A_2$ to get $R_{12}$. The next step is updating the rest of the matrix $A_2$. After that, we start a new panel factorization and repeat the steps until the last panel is factorized. See Fig~\ref{fig:bl_qr} for the intuitive steps.

\begin{figure}
\includegraphics[width=1.0\columnwidth]{blocking_qr.png} 
\caption{Blocking QR factorization steps}\label{fig:bl_qr}
\end{figure}

\subsubsection{Recursive Strategy}
Compared to the blocking strategy, the recursive strategy is not that popular. The reasons might be that blocking algorithms give enough data locality and parallelism on previous architectures; but now on the new matrix accelerators, it no longer can. The recursive algorithm is assembled like this equation~(\ref{eq:recqr}) and the workflow is: at first, we divide evenly its columns
into two halves, denoted by $A=[A_1 |  A_2]$. At first, we factorize the first half $A_1=Q_1R_{11}$,
and then compute north-east quarter of $R_{12}=Q_1^T A_2$. Next we update the
second half $A_2 = A_2 - Q_1 R_{12}$. Finally, QR factorize the updated second half
$A_2 = Q_2 R_{22}$. Note that the QR of the two halves can be recursed using this
algorithm itself. Fig~\ref{fig:re_qr} shows the general steps of factorizing a matrix with 2-level recursion.

\begin{equation}
[A_1 | A_2 ] = [Q_1 | Q_2 ] \left[ \begin{array}{c|c}
R_{11} & R_{12} \\ \hline
 & R_{22}
\end{array}   \right]\label{eq:recqr}
\end{equation}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{recursive_qr.png} 
\caption{Blocking QR factorization steps}\label{fig:re_qr}
\end{figure}

Zhang~\cite{QRTensorCore} has explained and shown the recursive QR performs better than the conventional QR on TensorCore. The reason behind the speedup is that recursive algorithms can provide larger GEMMs which can be executed more quickly on TensorCore, while the conventional algorithms cannot provide such GEMMs. Different from the in-core algorithms, the OOC algorithms focus more on the data movement and elaborate pipelining issues. So, the following subsections will discuss the effects on the data movement of such algorithms, by using OOC QR factorization as an example. 

\subsection{The Amount of Data Movement}
In the out-of-core scenario, the panel factorization will be performed on GPU step by step, while the GEMMs will be out-of-core and follow some specific computing pattern. But in this section, to simplify the computations, we'll only discuss the data movement of the naive implementation, that the elements will not be reused and it can give us an overview of the data movement overhead in the worst case.

\subsubsection{Blocking Strategy}

Given a matrix with size $m\times n$, where $m$ is the number of rows and $n$ is the number of columns. The panel size (blocksize) is $b$ and the matrix will need $k$ iterations to be fully factorized. Then we have $n=kb$. In the $i$-th iteration, the overall host to device data movement is $mb+mb+m(n-ib)+mb+b(n-ib)+m(n-ib)$, and the overall device to host data movement is $mb+b^2+b(n-ib)+m(n-ib)$. As a result, during the entire factorization, the amount of data movement from host to device is:
$$
\sum^k_{i=1}[3mb+(2m+b)(n-ib))]=(k+2)mn+\frac{n^2}{2}-\frac{nb}{2}
$$
And the amount of data movement from device back to host is:
$$
\sum^k_{i=1}[mb+b^2+(m+b)(n-ib)]=\frac{1}{2}[(k+1)mn+n^2+nb]
$$

\subsubsection{Recursive Strategy}
The amount of data movement in the recursive algorithm is a bit more complicated. First of all, the recursion depth is $\log_2^k$. Different from the blocking algorithm, only the deepest recursion does the factorization, while other levels of recursion perform GEMMs. So, the overall data movement of the deepest recursion is $mn$. For the recursions of GEMMs, the amount of data movement is $\sum^{\log_2^k-1}_i[2mn+2^{i-1}b^2]$. Hence, the total amount of data movement from host to device is:
$$
2(\log_2k+1)mn+\frac{mn}{2}-\frac{nb}{2}
$$

Similarly, the amount of data movement from device to host is:
$$
(\frac{1}{2}log_2^k+1)mn+\frac{n^2}{2}
$$

According to the quantitative analysis, it can be observed that both the amount of data movement of blocking and recursive OOC QR is highly related to the number of blocks $k$. Obviously, the data movement of the recursive QR is related to $\log k$, while the blocking QR is linearly related to $k$. This means the gap will become even larger as the $k$ increases. Note that this is only a rough estimation of the data movement because it's based on the assumption that all of the data won't be reused. In practice, some of the matrices can definitely be reused in the computations. But the results can still be a good reference to show the advantages of recursive algorithms, and we will also give the real quantitative data movement results in the evaluation section.

% \textbf{Is this practically optimal? Can we derive and 
% prove a lower bound of data movement and attain it?}

\subsection{Overlap Ratio in GEMMs}
In the above subsection, we've discussed the data movement behaviors and it has been shown that the recursive algorithm has less data movement. In this subsection, we'll try to explain that the recursive algorithm has advantages over the blocking algorithm with regard to GEMMs overlap ratio (here the overlap ratio means how much the data movement can be overlapped by computation).  Typically, there are two types GEMMs, one is an \textbf{inner product} (to simplify the name, we'll call the GEMM types in Fig~\ref{fig:re_inner},~\ref{fig:bl_inner} inner product) to generate $R_{12}=Q_1^TA_2$ and another is an \textbf{outer product} (Fig~\ref{fig:re_outer},~\ref{fig:bl_outer}) to update $A_2=A_2-Q_1R_{12}$. The overlap patterns of these two types are different, thus we'll discuss them separately. 

\subsubsection{Inner Product}
Suppose we have two matrices $A$ and $B$ with size $m\times k$ and $k\times n$. In the recursive QR factorization, $m$ equals to $n$. As the matrix is typically too large to store on the device, we may want to divide the matrices into several tiles. For each tile, we will move in and move out GPU memory to perform computations. To obtain the best performance, the time cost of data movement should be overlapped by the time cost of in-core TCGEMMs.

\begin{figure}
\includegraphics[width=1.0\columnwidth]{recursive_inner.png} 
\caption{Out-of-core inner product tiling strategy in recursive QR factorization}\label{fig:re_inner}
\end{figure}

To minimize the data movement in recursive QR factorization, we follow Fig~\ref{fig:re_inner} to divide the two matrices. In this case, the matrix $A$ and $B$ will be accessed only once. Then the data movement from device to host will cost $\frac{4mk+4nk}{R_m}$ and the TCGEMM will cost $\frac{2mnk}{R_g}$, where ${R_m}$ and ${R_g}$ denote the CPU-GPU data transferring rate and TCGEMM computation rate respectively. To hide the data movement cost by overlapping, $\frac{4mk+4nk}{R_m}$ must less than $\frac{2mnk}{R_g}$, which means $m$ should larger than $\frac{4R_g}{R_m}$. On V100 GPU, the $R_g$ is around 90TFLOPs and $R_m$ is around 12GB/s if using pinned memory. Then the final $m$ should be larger than 30,000. And this is usually the case for problems that require out-of-core computation.

\begin{figure}
\includegraphics[width=1.0\columnwidth]{blocking_inner.png} 
\caption{Out-of-core inner product tiling strategy in blocking QR factorization}\label{fig:bl_inner}
\end{figure}

When it comes to the blocking algorithm, we can let the matrix $A$ to be stored on GPU and divide $B$ into blocks with blocksize $b$, see~\ref{fig:bl_inner}. Therefore, the inequality becomes $\frac{4kb}{R_m}<\frac{2mkb}{R_g}\Longrightarrow m>\frac{2R_g}{R_m}$ and the final $m=15,000$.

It seems that the $m$ in blocking algorithm is easier to achieve, but one thing should be mentioned that the $m$ in blocking algorithm is typically relatively small and fixed to allow the panel to be stored on the device, especially when the GPU memory is limited and the number of columns of the original matrix is large. For example, if the GPU memory is 16GB and we want to factorize a matrix with size $131072\times 131072$, then the $m$ is typically less than 10,000 to avoid running out of GPU memory. In contrast, the $m$ in the recursive algorithm is more flexible and not related to the blocksize. Still, with a GPU has 16GB memory, factorize a $131072\times 131072$ matrix will lead to four inner products if the blocksize is 8192, including $8192\times 131072 \times 8192$, $16384\times 131072 \times 16384$, $32768\times 131072 \times 32768$ and $65536\times 131072 \times 65536$. At least the data movement in the largest two GEMMs can be perfectly overlapped (computing bound), while all of the GEMMs in blocking QR are memory bound. Regarding the smaller GEMMs in the recursive QR, we'll discuss how to optimize them in the next section.

\subsubsection{Outer Product}
Similarly,suppose the GEMM size is $m\times n\times k$ (matrices sizes are $m\times k$ and $k\times n$). For the outer product, we'll use a different strategy to minimize the data movement. 

\begin{figure}
\includegraphics[width=1.0\columnwidth]{recursive_outer.png} 
\caption{Out-of-core outer product tiling strategy in recursive QR factorization}\label{fig:re_outer}
\end{figure}

For the recursive algorithm, we can let the matrix $B$ to be stored on GPU, and we move the blocks with size $b\times k$ of matrix $A$ and $C$ in and out (Fig~\ref{fig:re_outer}). Then the data movement will cost $\frac{4bk+4bn}{R_m}$ and the TCGEMM will cost $\frac{2bkn}{R_g}$, thus we have $n>\frac{4R_g}{R_m}=3e4$, which is the same as previous result.

\begin{figure}
\includegraphics[width=1.0\columnwidth]{blocking_outer.png} 
\caption{Out-of-core outer product tiling strategy in blocking QR factorization}\label{fig:bl_outer}
\end{figure}

For the blocking algorithm, as the matrix $A$ and $B$ is tall and skinny, we can let both $A$ and $B$ to be stored on GPU to avoid unnecessary data movement. In this case, we can only care about the data movement of $C$, and the strategy is shown in Fig~\ref{fig:bl_outer}. The time cost of moving $C$ from CPU to GPU will take $\frac{4b_1b_2}{R_m}$ and we'll spend $\frac{2b_1kb_2}{R_g}$ on TCGEMM. Hence, the $k$ should be larger than 1.5e4 to overlap the data movement.

Although the strategy is different in the two GEMM types, the results are the same. Based on our analysis, we cannot expect the data movement of the blocking algorithm to be overlapped if we desire minimum data movement. Note that we didn't compute the time cost of moving data from GPU to CPU, as this kind of data movement can always be overlapped by moving data from CPU to GPU.

\subsection{Summary}
We've mentioned in the first section that in designing a high-performance OOC algorithm, the data movement should be reduced and the overlap ratio should be higher. From our above analysis, we could find that the recursive algorithm has advantages over the blocking algorithm because it can reduce data movement and increase data overlap ratio. Specifically, the performance of GEMMs in the recursive algorithm is less relevant to the blocksize, while the performance of GEMMs in the blocking algorithm is restricted by the blocksize. This means the recursive algorithm is more likely to perform even better on GPUs with smaller memory or larger scale matrices. In addition, we can also have more flexibility to optimize the large GEMMs in the recursive algorithm, because we don't really care about the blocksize. For the small GEMMs, we'll introduce the optimization strategy in the next section.  

\section{Implementation and Optimization}
We've already shown the recursive QR factorization has better data movement behaviors and overlapping ratio, the implementation and optimization are very important as well. So, in this section, we'll illustrate our implementation and optimization strategy. Note that we divide the optimization into two parts, the first part is the optimization inside GEMMs, which is called GEMM-level optimization; the second part is optimizing the data transferring between panel factorization and GEMMs, and it's termed as QR-level optimization. 

\subsection{GEMM-Level Implementation and Optimization}
Similar to the performance analysis, the two types of GEMMs have two different implementations.
\subsubsection{Inner Product}
Typically, to implement a high concurrent CPU-GPU program, we should be clear about how to use the streams. Cuda streams\footnote{\url{https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/}} are introduced by Nvidia to solve the concurrency problems of executing programs on the device. Generally speaking, with streams and proper devices (most modern Nvidia GPUs support CUDA streams), moving data from device to host, from host to device, and in-core computations can be executed at the same time. In our problem scenario, moving data from host to device, in-core computations, and moving data from device to host can work concurrently, because computations can be overlapped by communications, and the host-device communication links are bidirectional. Therefore, we need at least three streams to make these three assignments run in parallel. But because it's inner product, it's unnecessary to transfer the matrix $C$ from device to host before the entire computations is ended. Thus, we can only let the move-in operations and the computations be asynchronous, and move the data out in the end. In this case, we create several streams and each stream takes charge of one block. The final pipeline is shown in Fig~\ref{fig:rec_inner_time}.

\subsubsection{Outer Product}
In terms of this type of GEMM, as we discussed before, the $B$ is already stored on GPU (the $B$ is the result of the inner product, there's no need to copy it from CPU to GPU again). So, we only need to read tiles in $A$ and $C$ and write to the tiles in $C$. But because of the limited GPU memory, we use the same GPU memory space to store the matrix $C$, which means the move-in operation should be performed after finishing the entire move-out operation to avoid conflict. This waiting impedes the performance because the move-out operation cannot be overlapped. To solve this problem, we use extra memory space to temporally store the $C$ and once the move-out operation is done, we perform a fast in-core data transferring, and at the same time, we start another move-in operation, see Fig~\ref{fig:rec_outer_time}.

\subsubsection{Blocksize Matters}
The blocksize of GEMMs cannot be selected casually. Smaller blocksize leads to slower in-core computation, and larger blocksize will increase the overhead of the first move-in operation and the last move-out operation. So we can follow this strategy: start with a relatively small blocksize and gradually increase it to the max blocksize. Consider this situation, when we're computing the largest inner product in recursive QR with GEMM size $65536*131072*65536$, and we set the blocksize to be 8192. Then the first step is to compute a GEMM with a size of $65536*8192*65536$. Obviously, if we start with blocksize 8192, then the first move-in operation cannot be overlapped by computations. But if we start with blocksize 2048 and gradually increase the blocksize to 8192, then the same part of the move-in operation of the first block ($65536*8192$) can be overlapped. Actually, based on our experimental results, this trick increases the inner product performance from around 85TFLOPs to 87TFLOPs. 

\subsection{QR-Level Implementation and Optimization}
So far, the optimization of OOC GEMMs is done, but there are still some problems left. For example, how to deal with small GEMMs? The $m$ or $n$ in small GEMMs is not large enough so that the data movement cannot be overlapped perfectly. Indeed, we cannot optimize the small GEMMs anymore because of the insufficient data intensity, but we can tackle this problem in the QR-level optimization that can helps us avoid unnecessary data movement. The optimization doesn't happen inside GEMMs but works between QR panel factorization and the GEMMs. Besides, using the results from the inner product in the outer product to reduce unnecessary data movement can also be regarded as QR-level optimization to some extent.

So, after answering what's QR-level optimization and implementation, the question becomes how to implement and optimize it. Actually, the implementation is quite easy, following the design pattern of the in-core recursive QR~\cite{QRTensorCore} is the solution. But the difficulty is how to optimize it, including cutting off the unnecessary data movement and enabling
cross BLAS operation overlapping.

The first optimizing strategy is cutting off some move-in operations of the panel. Because when the GEMM size is small, the entire results can be stored on GPU and they can be used directly in the next panel factorization. Similarly, when the GEMM size is small, it doesn't really need to read the matrix $A$ from CPU, it can directly use the panel factorization results and only read $B$ from CPU.

The second optimizing strategy is trying to hide the move-out operations between panel factorization and GEMMs. For example, when the panel factorization is finished and the factorized matrix is ready to be transferred to host, we can perform a move-in operation for inner product at the same time. This kind of overlapping can also happen between inner product and outer product. When we're moving $R_{12}$ out, we can move in the first blocks of $A$ and $C$. Also, after the outer product, the last move-out operation can be overlapped by moving in the first few columns of the panel. See Fig~\ref{fig:rec_qr_time} for the intuitive optimization results.

\section{Experimental evaluation}
In this section, we're going to show our experimental results of the OOC QR factorization. We will evaluate the performance of the GEMMs and the entire QR factorization, by providing the overall execution time and the timelines. 

For all experiments we use a3.10.0-1160.11.1.el7.x86\_64 Linux operating system with 128GB memory. The GPU is NVIDIA V100 GPU (32GB) PCIe version. The CUDA version is 10.1, which contains a C++ compiler and cublas library. The in-core QR factorization is based on the LATER project.\footnote{\url{https://github.com/Orgline/LATER}}

\subsection{The Performance Behaviours of GEMMs}
As we discussed before, the shapes and sizes of GEMMs in the recursive QR and blocking QR are different, therefore, in this subsection, we'll study the performance behaviors of the largest GEMMs (because they cost a large portion of the entire QR factorization time) in recursion and blocking. Table~\ref{tbl:inner} and Table~\ref{tbl:outer} show the quantitative results of the GEMMs, including the time cost of data movement, GEMMs, overall time cost, and relative flops. Meanwhile, Fig~\ref{fig:blk_inner_time}, ~\ref{fig:rec_inner_time}, ~\ref{fig:blk_outer_time} and~\ref{fig:rec_outer_time} give the timelines of the different OOC GEMMs.

\subsubsection{Inner product performance}
In terms of inner product, in the previous analysis, we assume that all in-core computations run at 90TFLOPs. But the results suggest that the largest GEMM in blocking QR doesn't obey the rule, as the in-core GEMM only runs at 52.6TFLOPs, while the in-core recursive GEMM runs at 99.9TFLOPs. This is probably because of the special shape of the blocking in-core GEMM. Actually, in the TensorCore-based QR paper~\cite{QRTensorCore}, the authors also mention that tall and skinny GEMMs are very hard to run at peak speed on TensorCore. Fortunately, the in-core recursive GEMMs don't have this weak point when the GEMMs are large enough. No doubt that the small GEMMs will also have some problem, but after all, they're only a small part of the entire computations, while the blocking GEMMs need to face this problem in the complete period because it consists of fixed small-sized and shaped GEMMs which are inefficient on TensorCore.

Both timelines of the inner product (Fig~\ref{fig:blk_inner_time},\ref{fig:rec_inner_time}) show a good overlap rate. In other words, the time cost of close to peak GEMMs dominate the entire computations.
\begin{table}[h]

\centering
\begin{tabular}{ |c|c|c|c| } 
\hline
Single Block Time Cost &  Recursive & Blocking \\
 \hline
 Host to device & 693ms  & 728ms   \\
\hline
 GEMM & 1408ms &  1337ms   \\
\hline
 Device to Host & 1306ms   &  81ms  \\
\hline

In-core flops  &99.9TFLOPs & 52.6TFLOPs \\
\hline
\hline
Overall Time cost &  Recursive & Blocking \\
 \hline

  Synchronous  & 18183ms  & 14920ms  \\
  \hline
 Synchronous flops  &  62.0TFLOPs & 33.0TFLOPs  \\
\hline
 Asynchronous  &  12932ms & 11286ms \\
\hline
 Asynchronous flops  & 87.1TFLOPs  &  43.6TFLOPs \\
\hline
\end{tabular}
\caption{Inner product behaviors, recursive matrix size is 65536*131072*65536 with blocksize 16384, blocking matrix size is 16384*131072*114688 with blocksize 16384}
\label{tbl:inner}
\end{table}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{blk_inner_time.png} 
\caption{The timeline of computing max inner product GEMM in 0.13M*0.13M in blocking QR factorization, the matrix size is 16384*131072*114688, the blocksize is 16384. }\label{fig:blk_inner_time}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{rec_inner_time.png} 
\caption{The timeline of computing max inner product GEMM in 0.13M*0.13M in recursive QR factorization, the matrix size is 65536*131072*65536, the blocksize is 16384.}\label{fig:rec_inner_time}
\end{figure}


\subsubsection{Outer product performance}

When it comes to the outer product performance, things become slightly different. When the QR blocksize is 16384, there's no big difference between the two types of GEMMs. Both the in-core GEMM and the overlap rate are desirable. But because the experiments are performed on V100 GPU which has 32GB device memory, the QR blocksize can be as large as 16384 so that the data movement is able to be overlapped perfectly based on our analysis. 

However, most of the widely-used GPUs, such as RTX 20,30 series, don't have such a big device memory. On these GPUs, the QR blocksize need to be reduced as a compromise. For example, Fig~\ref{fig:blk_outer_time_8192} shows the timeline of the blocking GEMMs with QR blocksize 8192, and we can see that the GEMMs cannot be overlapped anymore. Quantitatively, the time cost of host to device, GEMM, and device to host is 347ms, 170ms, 326ms. The computation time is dominated by the data movement, thus even if the total number of the flop of blocking GEMMs with QR blocksize 8192 is around 2x less than the GEMMs with QR blocksize 16384, the computations still spend more time.

\begin{table}[h]

\centering
\begin{tabular}{ |c|c|c|c| } 
\hline
Single Block Time Cost &  Recursive & Blocking \\
 \hline
 Host to device & 347ms  & 86ms   \\
\hline
 GEMM & 654ms &  89ms   \\
\hline
 Device to Host & 163ms   &  81ms  \\
\hline

In-core flops  & 107.6TFLOPs& 98.8TFLOPs \\
\hline
\hline
Overall Time cost &  Recursive & Blocking \\
 \hline

  Synchronous  & 14129ms  & 5119ms  \\
  \hline
 Synchronous flops & 60.3TFLOPs  & 34.7TFLOPs  \\
\hline
 Asynchronous  &  11517ms & 11286ms \\
\hline
 Asynchronous flops  & 97.7TFLOPs  &  96.2TFLOPs \\
\hline
\end{tabular}
\caption{Outer product behaviours, recursive matrix size is 131072*65536*65536 with blocksize 8192, blocking matrix size is 131072*16384*114688 with blocksize 16384 and 16384}
\label{tbl:outer}
\end{table}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{blk_outer_time.png} 
\caption{The timeline of computing max outer product GEMM in 0.13M*0.13M in blocking QR factorization, the matrix size is 131072*16384*114688, the blocksize $b_1, b_2$ is 16384 and 16384.}\label{fig:blk_outer_time}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{rec_outer_time.png} 
\caption{The timeline of computing max outer product GEMM in 0.13M*0.13M in recursive QR factorization, the matrix size is 131072*65536*65536, the blocksize is 8192}\label{fig:rec_outer_time}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{blk_outer_8192.png} 
\caption{The timeline of computing outer product GEMM with QR blocksize 8192, the matrix size is 131072*16384*131072, the inside GEMM blocksize $b_1, b_2$ is 32768 and 32768.}\label{fig:blk_outer_time_8192}
\end{figure}

In fact, the implementations of the GEMMs can be said to be optimal. For example, in terms of the outer product, our OOC GEMM will cost 11,517ms in total and the in-core computation costs 654*16=10,464ms. Add the first move-in and the last move-out time consumption (because they cannot be overlapped at all), we get the ideal time cost is 10,974ms. This is a fixed upper bound of the given GEMM with blocksize 8192. But there's no doubt that in real computations, some time will be wasted due to the delays and synchronizations, so the gap (543ms) is reasonable. In other words, the GEMMs are optimal.


\subsection{The Performance Behaviour of QR}
The performance of the two types of QR factorization is highly affected by the GEMMs because the in-core panel factorization is exactly the same. This means the recursive algorithm has big advantages over blocking algorithms because it has faster GEMMs. With regard to the QR-level optimization and overlapping, according to our analysis, the acceleration from reducing data movement and overlapping is very similar.

Fig~\ref{fig:blk_qr_time} and Fig~\ref{fig:rec_qr_time} demonstrate the timeline of the entire QR factorization. It's obvious that with QR blocksize 16384, the recursive QR takes advantage of the higher rate of inner product thereby beats the blocking QR factorization. We can also find the QR-level optimization helps the two factorization gain around 15\% speedup. 

However, when the blocksize is 8192, the performance of blocking QR goes down significantly due to the increasing time cost of GEMMs, as the data movement cannot be overlapped by the in-core GEMMs anymore (the time cost is 347ms,	170ms, 326ms for move-in, GEMM, move-out respectively). In contrast, the performance of recursive QR doesn't change much. It's not surprising to see this result, because the panel factorization is recursive, therefore, the only difference is the amount of data movement (can be overlapped mostly). We simulate the factorization by limiting the memory usage to be less than 16GB on V100 GPU and the results are shown in Fig~\ref{fig:blk_qr_time_8192},~\ref{fig:rec_qr_time_8192}. The speedup of recursive QR factorization is nearly 2x compared to the blocking QR factorization. The results also suggest that the higher ratio computation speed/memory capacity is, the more advantageous recursive vs. blocking is.


Another obvious difference between the two factorization is the amount of the overall data movement. See Table~\ref{tbl:data_mv}. The results can verify our claim that the recursive algorithm has less data movement than the blocking algorithm. Actually, the data movement is not very important here, because most data movement can be overlapped. But it's still valuable to see such difference because in some specific situations where moving data can not be hidden, the recursive algorithm still can help. 
\begin{table}
\centering
\begin{tabular}{ |c|c|c|c| } 
\hline
Data movement time &  Recursive & Blocking \\
 \hline
 Host to device & 37.9s  &  47.2s  \\
\hline
 Device to Host & 19.3s   &  22.3s  \\
\hline
\end{tabular}
\caption{The time cost of data movement of two different types of QR factorization with QR blocksize 16384}
\label{tbl:data_mv}
\end{table}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{blk_qr_time.png} 
\caption{The timeline of computing blocking out-of-core QR, the blocksize is 16384. }\label{fig:blk_qr_time}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{rec_qr_time.png} 
\caption{The timeline of computing recursive out-of-core QR, the blocksize is 16384. }\label{fig:rec_qr_time}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{blk_qr_time_8192.png} 
\caption{The timeline of computing blocking out-of-core QR, the blocksize is 8192. }\label{fig:blk_qr_time_8192}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{rec_qr_time_8192.png} 
\caption{The timeline of computing recursive out-of-core QR, the blocksize is 8192. }\label{fig:rec_qr_time_8192}
\end{figure}

To make our experiments more complete, we also experiment with different matrix shapes and sizes. However, limited by our main memory capacity, we only tested the matrices with sizes 65536*65536 and 262144*65536. Table~\ref{tbl:qr_comparison} demonstrates the time cost of the GEMMs and panel when we're using the blocking and the recursive strategy with blocksize 8192, respectively. In terms of the overall performance (Panel+GEMMs+data movement), we get 1.5x and 1.7x speedup respectively. As we can see in the Table~\ref{tbl:qr_comparison}, the time cost of the panel is the same among the two strategies, because we're using the same in-core panel factorization. The time cost of the GEMMs show significant differences, for the reason that the recursive strategy can provide larger data overlap ratio and better matrices' shapes to be performed on TensorCore. However, we can observe that the speedup is not as high as the size 131072*131072 (2.0x) because of two reasons: 1) the size 131072*131072 has larger proportion of GEMMs, which means it has more opportunities to enjoy the faster OOC GEMMs; 2) the matrix shapes are easier to be accelerated on TensorCore (the GEMMs in the size 262144*65536 are taller and thinner). Based on the these experiments, we can conclude that the recursive QR factorization usually show a great advantage over the blocking algorithm on different sizes, and the larger and the more square matrices the matrices are the better.
% The inside OOC GEMMs in recursive QR factorization still show a great advantage over the blocking algorithm. 

\begin{table}
\centering
\begin{tabular}{ |c|c|c|c| } 
\hline
 Partition &  Recursive & Blocking \\
\hline
\hline
Matrix Size & 65536*65536 &  \\
 \hline
 GEMMs & 10.5s  &  18.9s  \\
\hline
 Panel & 2.7s   &  2.7s  \\
\hline
\hline
Matrix Size & 262144*65536 &\\
 
 \hline
 GEMMs & 38.5s  & 77.0s  \\
\hline
 Panel & 9.0s   &  9.0s  \\
\hline
\end{tabular}
\caption{The total time cost of GEMMs and panel of two different sizes of QR factorization (65536*65536 and 262144*65536) with blocksize 8192.}
\label{tbl:qr_comparison}
\end{table}


\subsection{Summary}
In this section, we've shown the experimental results that can support our previous claim: 1) The data movement in recursive QR is less than the data movement in blocking QR; 2) The blocksize of QR factorization needs to be larger than 15k to overlap data movement perfectly; 3)the recursive QR has great advantages over blocking QR in terms of speed. 

Generally speaking, the recursive OOC QR is around 1.25x faster than the conventional blocking QR on GPUs with larger device memory, and around 2x faster than blocking QR when the memory is small. 

\section{Conclusion and Future work}
In this paper, we've discussed the recursive out-of-core QR factorization that shows better performance over blocking out-of-core QR factorization. We've implemented the recursive QR factorization and the experiments imply that such acceleration is generally from the inside GEMMs. We've also analyzed the reasons behind such speedup. In short, the GEMMs in conventional blocking QR factorization cannot run at peak performance on TensorCore, neither overlap the data movement by in-core computations due to the fixed blocksize, while the GEMMs in recursive QR factorization is insensitive to the blocksize, in fact, it provides dynamically adjusted blocksizes. Hence, they're more flexible and easier to be optimized thereby 
can perform faster.

It's interesting to think about if this kind of strategy can be applied to other applications such as LU and Cholesky factorization. As there's no in-core TensorCore based partial pivoted LU and Cholesky factorization, we can only analyze it theoretically. The pattern of out-of-core LU and Cholesky factorization is very similar to the out-of-core QR factorization, which is interleaving panel factorization and trailing matrix update. For example, the trailing matrix update in LU factorization is also of outer product form, and the recursive algorithm can definitely help this kind of GEMMs. 
Anyway, the other out-of-core linear algebra algorithms are included in our future work.

Another attempt that will be considered in the future is trying to deploy the recursive algorithms on A100 GPU, whose peak performance of TensorCore is over 300TFLOPS. In this situation, the blocksize must be larger than 60k if we want the inside GEMMs to be computation-dominated. So, it becomes impossible for blocking algorithms to have such a big blocksize due to the limited memory space. But for recursive algorithms, the dominant large GEMMs can run at full speed, which indicates the recursive algorithm might be even more advantageous than on V100. More accessible architectures such as RTX20,30 series with slightly reduced computation speed and much reduced memory capacity will see a bigger advantage of recursion as well for the same reason. Going forward, the gap between computation speed and data movement speed/memory capacity is likely going to continue to increase, in which cases conventional blocking will be increasingly data movement constrained and has ever lower efficiency. 
\bibliographystyle{ACM-Reference-Format}
\bibliography{all,references}


\end{document}
